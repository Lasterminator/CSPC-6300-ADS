{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def save_results_to_file(results, output_dir, timestamp):\n",
    "    \"\"\"Save results directly to files without using pandas\"\"\"\n",
    "    # Save detailed JSON results\n",
    "    json_path = os.path.join(output_dir, f\"detailed_results_{timestamp}.json\")\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    csv_path = os.path.join(output_dir, f\"summary_{timestamp}.csv\")\n",
    "    with open(csv_path, 'w') as f:\n",
    "        f.write(\"filename,status,text_length,best_method\\n\")\n",
    "        # Write data\n",
    "        for r in results:\n",
    "            text_length = len(r['text']) if r['status'] == 'success' and 'text' in r else 0\n",
    "            best_method = r.get('best_method', '')\n",
    "            f.write(f\"{r['filename']},{r['status']},{text_length},{best_method}\\n\")\n",
    "            \n",
    "class EnhancedOCR:\n",
    "    def __init__(self, input_folder, output_folder, num_threads=4):\n",
    "        \"\"\"Initialize OCR processor\"\"\"\n",
    "        self.input_folder = input_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.num_threads = num_threads\n",
    "        self.create_output_dirs()\n",
    "\n",
    "    def create_output_dirs(self):\n",
    "        \"\"\"Create output directory structure\"\"\"\n",
    "        self.processed_dir = os.path.join(self.output_folder, 'processed_images')\n",
    "        self.results_dir = os.path.join(self.output_folder, 'results')\n",
    "        os.makedirs(self.processed_dir, exist_ok=True)\n",
    "        os.makedirs(self.results_dir, exist_ok=True)\n",
    "\n",
    "    def enhance_image(self, image):\n",
    "        \"\"\"Advanced image preprocessing pipeline\"\"\"\n",
    "        try:\n",
    "            if len(image.shape) == 3:\n",
    "                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            else:\n",
    "                gray = image.copy()\n",
    "\n",
    "            max_dim = 2000\n",
    "            height, width = gray.shape\n",
    "            if max(height, width) > max_dim:\n",
    "                scale = max_dim / max(height, width)\n",
    "                gray = cv2.resize(gray, None, fx=scale, fy=scale)\n",
    "\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "            enhanced = clahe.apply(gray)\n",
    "\n",
    "            denoised = cv2.fastNlMeansDenoising(enhanced)\n",
    "\n",
    "            binary_adaptive = cv2.adaptiveThreshold(\n",
    "                denoised, 255, \n",
    "                cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                cv2.THRESH_BINARY, 11, 2\n",
    "            )\n",
    "\n",
    "            _, binary_otsu = cv2.threshold(\n",
    "                denoised, 0, 255, \n",
    "                cv2.THRESH_BINARY + cv2.THRESH_OTSU\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                'original': gray,\n",
    "                'enhanced': enhanced,\n",
    "                'binary_adaptive': binary_adaptive,\n",
    "                'binary_otsu': binary_otsu\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in image enhancement: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_text(self, image_dict):\n",
    "        \"\"\"Extract text using different preprocessing variants\"\"\"\n",
    "        results = {}\n",
    "        custom_config = r'--oem 3 --psm 6'\n",
    "        \n",
    "        for img_type, img in image_dict.items():\n",
    "            try:\n",
    "                text = pytesseract.image_to_string(img, config=custom_config)\n",
    "                results[img_type] = text.strip()\n",
    "            except Exception as e:\n",
    "                results[img_type] = f\"Error: {str(e)}\"\n",
    "                \n",
    "        return results\n",
    "\n",
    "    def process_single_image(self, image_file):\n",
    "        \"\"\"Process a single image through the pipeline\"\"\"\n",
    "        try:\n",
    "            image_path = os.path.join(self.input_folder, image_file)\n",
    "            image = cv2.imread(image_path)\n",
    "            \n",
    "            if image is None:\n",
    "                return {\n",
    "                    'filename': image_file,\n",
    "                    'status': 'error',\n",
    "                    'error': 'Failed to load image'\n",
    "                }\n",
    "\n",
    "            enhanced_images = self.enhance_image(image)\n",
    "            if enhanced_images is None:\n",
    "                return {\n",
    "                    'filename': image_file,\n",
    "                    'status': 'error',\n",
    "                    'error': 'Failed to enhance image'\n",
    "                }\n",
    "\n",
    "            for img_type, img in enhanced_images.items():\n",
    "                output_path = os.path.join(\n",
    "                    self.processed_dir,\n",
    "                    f\"{os.path.splitext(image_file)[0]}_{img_type}.png\"\n",
    "                )\n",
    "                cv2.imwrite(output_path, img)\n",
    "\n",
    "            text_results = self.extract_text(enhanced_images)\n",
    "\n",
    "            best_result = max(text_results.items(), key=lambda x: len(x[1]))\n",
    "\n",
    "            return {\n",
    "                'filename': image_file,\n",
    "                'status': 'success',\n",
    "                'best_method': best_result[0],\n",
    "                'text': best_result[1],\n",
    "                'all_results': text_results\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'filename': image_file,\n",
    "                'status': 'error',\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "    def process_all_images(self):\n",
    "        \"\"\"Process all images using thread pool\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        image_files = [f for f in os.listdir(self.input_folder) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tiff'))]\n",
    "        \n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=self.num_threads) as executor:\n",
    "            futures = {\n",
    "                executor.submit(self.process_single_image, image_file): image_file \n",
    "                for image_file in image_files\n",
    "            }\n",
    "            \n",
    "            with tqdm(total=len(image_files), desc=\"Processing images\") as pbar:\n",
    "                for future in as_completed(futures):\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                    pbar.update(1)\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        save_results_to_file(results, self.results_dir, timestamp)\n",
    "\n",
    "        processing_time = time.time() - start_time\n",
    "        stats = {\n",
    "            'total_images': len(image_files),\n",
    "            'successful': len([r for r in results if r['status'] == 'success']),\n",
    "            'failed': len([r for r in results if r['status'] == 'error']),\n",
    "            'processing_time': processing_time,\n",
    "            'average_time_per_image': processing_time / len(image_files)\n",
    "        }\n",
    "\n",
    "        stats_path = os.path.join(self.results_dir, f\"processing_stats_{timestamp}.json\")\n",
    "        with open(stats_path, 'w') as f:\n",
    "            json.dump(stats, f, indent=4)\n",
    "\n",
    "        return stats, results\n",
    "    \n",
    "input_folder = \"images\"\n",
    "output_folder = \"output\"\n",
    "\n",
    "ocr = EnhancedOCR(input_folder, output_folder, num_threads=4)\n",
    "stats, results = ocr.process_all_images()\n",
    "\n",
    "print(\"\\nProcessing Summary:\")\n",
    "print(f\"Total images processed: {stats['total_images']}\")\n",
    "print(f\"Successful: {stats['successful']}\")\n",
    "print(f\"Failed: {stats['failed']}\")\n",
    "print(f\"Total processing time: {stats['processing_time']:.2f} seconds\")\n",
    "print(f\"Average time per image: {stats['average_time_per_image']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   1%|          | 5/427 [00:05<05:56,  1.18it/s]/home/tpittal/.local/lib/python3.11/site-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Processing images: 100%|██████████| 427/427 [48:21<00:00,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Summary:\n",
      "Total images processed: 427\n",
      "Successful: 427\n",
      "Failed: 0\n",
      "Total processing time: 2901.19 seconds\n",
      "Average time per image: 6.79 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import easyocr\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def save_results_to_file(results, output_dir, timestamp):\n",
    "    \"\"\"Save results directly to files without using pandas\"\"\"\n",
    "    # Save detailed JSON results\n",
    "    json_path = os.path.join(output_dir, f\"detailed_results_{timestamp}.json\")\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    csv_path = os.path.join(output_dir, f\"summary_{timestamp}.csv\")\n",
    "    with open(csv_path, 'w') as f:\n",
    "        f.write(\"filename,status,text_length,best_method\\n\")\n",
    "        # Write data\n",
    "        for r in results:\n",
    "            text_length = len(r['text']) if r['status'] == 'success' and 'text' in r else 0\n",
    "            best_method = r.get('best_method', '')\n",
    "            f.write(f\"{r['filename']},{r['status']},{text_length},{best_method}\\n\")\n",
    "            \n",
    "class EnhancedOCR:\n",
    "    def __init__(self, input_folder, output_folder, num_threads=4):\n",
    "        \"\"\"Initialize OCR processor with EasyOCR\"\"\"\n",
    "        self.input_folder = input_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.num_threads = num_threads\n",
    "        self.reader = easyocr.Reader(['en'], gpu=True)  # Initialize EasyOCR Reader\n",
    "        self.create_output_dirs()\n",
    "\n",
    "    def create_output_dirs(self):\n",
    "        \"\"\"Create output directory structure\"\"\"\n",
    "        self.processed_dir = os.path.join(self.output_folder, 'processed_images')\n",
    "        self.results_dir = os.path.join(self.output_folder, 'results')\n",
    "        os.makedirs(self.processed_dir, exist_ok=True)\n",
    "        os.makedirs(self.results_dir, exist_ok=True)\n",
    "\n",
    "    def enhance_image(self, image):\n",
    "        \"\"\"Advanced image preprocessing pipeline\"\"\"\n",
    "        try:\n",
    "            if len(image.shape) == 3:\n",
    "                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            else:\n",
    "                gray = image.copy()\n",
    "\n",
    "            max_dim = 2000\n",
    "            height, width = gray.shape\n",
    "            if max(height, width) > max_dim:\n",
    "                scale = max_dim / max(height, width)\n",
    "                gray = cv2.resize(gray, None, fx=scale, fy=scale)\n",
    "\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "            enhanced = clahe.apply(gray)\n",
    "\n",
    "            denoised = cv2.fastNlMeansDenoising(enhanced)\n",
    "\n",
    "            binary_adaptive = cv2.adaptiveThreshold(\n",
    "                denoised, 255, \n",
    "                cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                cv2.THRESH_BINARY, 11, 2\n",
    "            )\n",
    "\n",
    "            _, binary_otsu = cv2.threshold(\n",
    "                denoised, 0, 255, \n",
    "                cv2.THRESH_BINARY + cv2.THRESH_OTSU\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                'original': gray,\n",
    "                'enhanced': enhanced,\n",
    "                'binary_adaptive': binary_adaptive,\n",
    "                'binary_otsu': binary_otsu\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in image enhancement: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_text(self, image_dict):\n",
    "        \"\"\"Extract text using EasyOCR\"\"\"\n",
    "        results = {}\n",
    "        for img_type, img in image_dict.items():\n",
    "            try:\n",
    "                # EasyOCR requires images in RGB format\n",
    "                if len(img.shape) == 2:  # If grayscale, convert to RGB\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "                text_result = self.reader.readtext(img, detail=0)\n",
    "                results[img_type] = \" \".join(text_result).strip()\n",
    "            except Exception as e:\n",
    "                results[img_type] = f\"Error: {str(e)}\"\n",
    "        return results\n",
    "\n",
    "    def process_single_image(self, image_file):\n",
    "        \"\"\"Process a single image through the pipeline\"\"\"\n",
    "        try:\n",
    "            image_path = os.path.join(self.input_folder, image_file)\n",
    "            image = cv2.imread(image_path)\n",
    "            \n",
    "            if image is None:\n",
    "                return {\n",
    "                    'filename': image_file,\n",
    "                    'status': 'error',\n",
    "                    'error': 'Failed to load image'\n",
    "                }\n",
    "\n",
    "            enhanced_images = self.enhance_image(image)\n",
    "            if enhanced_images is None:\n",
    "                return {\n",
    "                    'filename': image_file,\n",
    "                    'status': 'error',\n",
    "                    'error': 'Failed to enhance image'\n",
    "                }\n",
    "\n",
    "            for img_type, img in enhanced_images.items():\n",
    "                output_path = os.path.join(\n",
    "                    self.processed_dir,\n",
    "                    f\"{os.path.splitext(image_file)[0]}_{img_type}.png\"\n",
    "                )\n",
    "                cv2.imwrite(output_path, img)\n",
    "\n",
    "            text_results = self.extract_text(enhanced_images)\n",
    "\n",
    "            best_result = max(text_results.items(), key=lambda x: len(x[1]))\n",
    "\n",
    "            return {\n",
    "                'filename': image_file,\n",
    "                'status': 'success',\n",
    "                'best_method': best_result[0],\n",
    "                'text': best_result[1],\n",
    "                'all_results': text_results\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'filename': image_file,\n",
    "                'status': 'error',\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "    def process_all_images(self):\n",
    "        \"\"\"Process all images using thread pool\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        image_files = [f for f in os.listdir(self.input_folder) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tiff'))]\n",
    "        \n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=self.num_threads) as executor:\n",
    "            futures = {\n",
    "                executor.submit(self.process_single_image, image_file): image_file \n",
    "                for image_file in image_files\n",
    "            }\n",
    "            \n",
    "            with tqdm(total=len(image_files), desc=\"Processing images\") as pbar:\n",
    "                for future in as_completed(futures):\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                    pbar.update(1)\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        save_results_to_file(results, self.results_dir, timestamp)\n",
    "\n",
    "        processing_time = time.time() - start_time\n",
    "        stats = {\n",
    "            'total_images': len(image_files),\n",
    "            'successful': len([r for r in results if r['status'] == 'success']),\n",
    "            'failed': len([r for r in results if r['status'] == 'error']),\n",
    "            'processing_time': processing_time,\n",
    "            'average_time_per_image': processing_time / len(image_files) if image_files else 0\n",
    "        }\n",
    "\n",
    "        stats_path = os.path.join(self.results_dir, f\"processing_stats_{timestamp}.json\")\n",
    "        with open(stats_path, 'w') as f:\n",
    "            json.dump(stats, f, indent=4)\n",
    "\n",
    "        return stats, results\n",
    "    \n",
    "input_folder = \"images\"\n",
    "output_folder = \"output1\"\n",
    "\n",
    "ocr = EnhancedOCR(input_folder, output_folder, num_threads=4)\n",
    "stats, results = ocr.process_all_images()\n",
    "\n",
    "print(\"\\nProcessing Summary:\")\n",
    "print(f\"Total images processed: {stats['total_images']}\")\n",
    "print(f\"Successful: {stats['successful']}\")\n",
    "print(f\"Failed: {stats['failed']}\")\n",
    "print(f\"Total processing time: {stats['processing_time']:.2f} seconds\")\n",
    "print(f\"Average time per image: {stats['average_time_per_image']:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
