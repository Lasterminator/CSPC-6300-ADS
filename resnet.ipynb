{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Vocabulary size: 112\n",
      "Total samples: 427\n",
      "\n",
      "Model Summary:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "        Identity-174                 [-1, 2048]               0\n",
      "          ResNet-175                 [-1, 2048]               0\n",
      "          Linear-176                  [-1, 256]         524,544\n",
      "            ReLU-177                  [-1, 256]               0\n",
      "         Dropout-178                  [-1, 256]               0\n",
      "             GRU-179  [[-1, 128, 512], [-1, 2, 256]]               0\n",
      "          Linear-180             [-1, 128, 112]          57,456\n",
      "================================================================\n",
      "Total params: 24,090,032\n",
      "Trainable params: 24,090,032\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 30.70\n",
      "Params size (MB): 91.90\n",
      "Estimated Total Size (MB): 123.17\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 22/22 [00:46<00:00,  2.10s/it, loss=2.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8634\n",
      "Val Loss: 2.6259\n",
      "Saved best model\n",
      "\n",
      "Sample Predictions:\n",
      "Predicted: eeeee                                                                                                                           \n",
      "Predicted: eeeeeee                                                                                                                         \n",
      "Predicted: eeeeeeee                                                                                                                        \n",
      "\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 22/22 [00:46<00:00,  2.10s/it, loss=2.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.6129\n",
      "Val Loss: 2.6067\n",
      "Saved best model\n",
      "\n",
      "Sample Predictions:\n",
      "Predicted: ee                                                                                                                              \n",
      "Predicted: ee                                                                                                                              \n",
      "Predicted: ee                                                                                                                              \n",
      "\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 22/22 [00:46<00:00,  2.11s/it, loss=2.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.6064\n",
      "Val Loss: 2.6103\n",
      "\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 22/22 [00:45<00:00,  2.06s/it, loss=2.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.6047\n",
      "Val Loss: 2.6003\n",
      "Saved best model\n",
      "\n",
      "Sample Predictions:\n",
      "Predicted: ee                                                                                                                              \n",
      "Predicted: ee                                                                                                                              \n",
      "Predicted: ee                                                                                                                              \n",
      "\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 22/22 [00:45<00:00,  2.06s/it, loss=2.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.5948\n",
      "Val Loss: 2.6066\n",
      "\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 22/22 [00:45<00:00,  2.08s/it, loss=2.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.5959\n",
      "Val Loss: 2.5996\n",
      "Saved best model\n",
      "\n",
      "Sample Predictions:\n",
      "Predicted: ee                                                                                                                              \n",
      "Predicted: ee                                                                                                                              \n",
      "Predicted: ee                                                                                                                              \n",
      "\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 22/22 [00:46<00:00,  2.14s/it, loss=2.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.5963\n",
      "Val Loss: 2.5947\n",
      "Saved best model\n",
      "\n",
      "Sample Predictions:\n",
      "Predicted: ee                                                                                                                              \n",
      "Predicted: ee                                                                                                                              \n",
      "Predicted: ee                                                                                                                              \n",
      "\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 22/22 [00:45<00:00,  2.07s/it, loss=2.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.5883\n",
      "Val Loss: 2.5928\n",
      "Saved best model\n",
      "\n",
      "Sample Predictions:\n",
      "Predicted: ee                                                                                                                              \n",
      "Predicted: ee                                                                                                                              \n",
      "Predicted: ee                                                                                                                              \n",
      "\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 22/22 [00:45<00:00,  2.07s/it, loss=2.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.5864\n",
      "Val Loss: 2.5938\n",
      "\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 22/22 [00:45<00:00,  2.08s/it, loss=2.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.5965\n",
      "Val Loss: 2.5923\n",
      "Saved best model\n",
      "\n",
      "Sample Predictions:\n",
      "Predicted: ee                                                                                                                              \n",
      "Predicted: ee                                                                                                                              \n",
      "Predicted: ee                                                                                                                              \n",
      "\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 22/22 [00:45<00:00,  2.06s/it, loss=2.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.5855\n",
      "Val Loss: 2.5927\n",
      "\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 22/22 [00:46<00:00,  2.10s/it, loss=2.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.5890\n",
      "Val Loss: 2.5930\n",
      "\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 22/22 [00:44<00:00,  2.04s/it, loss=2.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.5859\n",
      "Val Loss: 2.5913\n",
      "Saved best model\n",
      "\n",
      "Sample Predictions:\n",
      "Predicted: ee                                                                                                                              \n",
      "Predicted: ee                                                                                                                              \n",
      "Predicted: ee                                                                                                                              \n",
      "\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 22/22 [00:46<00:00,  2.12s/it, loss=3.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.6066\n",
      "Val Loss: 2.5921\n",
      "\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 22/22 [00:46<00:00,  2.12s/it, loss=2.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.5900\n",
      "Val Loss: 2.6015\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Dataset Definition\n",
    "class HandwritingDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotations_file, max_length=128):\n",
    "        self.image_dir = image_dir\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        with open(annotations_file, 'r') as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        self.data = [\n",
    "            item for item in self.annotations \n",
    "            if item['status'] == 'success' and len(item['text'].strip()) > 0\n",
    "        ]\n",
    "\n",
    "        self.create_char_mappings()\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomRotation(degrees=10),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def create_char_mappings(self):\n",
    "        all_chars = set()\n",
    "        for item in self.data:\n",
    "            all_chars.update(item['text'])\n",
    "        \n",
    "        self.char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(all_chars))}\n",
    "        self.char_to_idx['<pad>'] = 0\n",
    "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
    "        self.vocab_size = len(self.char_to_idx)\n",
    "        \n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image_path = os.path.join(\n",
    "            self.image_dir,\n",
    "            f\"{os.path.splitext(item['filename'])[0]}_binary_adaptive.png\"\n",
    "        )\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Could not load image: {image_path}\")\n",
    "        if len(image.shape) == 2:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        image = self.transform(image)\n",
    "        text = item['text'][:self.max_length]\n",
    "        text_indices = [self.char_to_idx[c] for c in text]\n",
    "        if len(text_indices) < self.max_length:\n",
    "            text_indices.extend([0] * (self.max_length - len(text_indices)))\n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': torch.tensor(text_indices, dtype=torch.long),\n",
    "            'length': len(text)\n",
    "        }\n",
    "\n",
    "# Model Definition\n",
    "class HandwritingRecognitionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=256, sequence_length=128):\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.cnn = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        self.feature_processor = nn.Sequential(\n",
    "            nn.Linear(2048, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=3,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        features = self.cnn(x)\n",
    "        features = self.feature_processor(features)\n",
    "        features = features.unsqueeze(1).repeat(1, self.sequence_length, 1)\n",
    "        rnn_out, _ = self.rnn(features)\n",
    "        logits = self.fc(rnn_out)\n",
    "        return logits\n",
    "\n",
    "# Edit Distance Calculation\n",
    "def edit_distance(s1, s2):\n",
    "    \"\"\"Calculate edit distance using dynamic programming.\"\"\"\n",
    "    m, n = len(s1), len(s2)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if s1[i - 1] == s2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n",
    "    \n",
    "    return dp[m][n]\n",
    "\n",
    "def similarity_ratio(s1, s2):\n",
    "    \"\"\"Calculate similarity ratio based on edit distance.\"\"\"\n",
    "    distance = edit_distance(s1, s2)\n",
    "    max_len = max(len(s1), len(s2))\n",
    "    if max_len == 0:\n",
    "        return 1.0\n",
    "    return 1 - distance / max_len\n",
    "\n",
    "# Training Loop\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    with tqdm(train_loader, desc='Training') as pbar:\n",
    "        for batch in pbar:\n",
    "            images = batch['image'].to(device)\n",
    "            texts = batch['text'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            B, S, V = logits.shape\n",
    "            loss = criterion(logits.view(B * S, V), texts.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Validation Loop\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            texts = batch['text'].to(device)\n",
    "            logits = model(images)\n",
    "            B, S, V = logits.shape\n",
    "            loss = criterion(logits.view(B * S, V), texts.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "# Decode Predictions\n",
    "def decode_prediction(logits, idx_to_char, beam_width=5):\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    decoded_texts = []\n",
    "    for pred in predictions:\n",
    "        text = ''.join([idx_to_char[idx.item()] for idx in pred if idx.item() != 0])\n",
    "        decoded_texts.append(text)\n",
    "    return decoded_texts\n",
    "\n",
    "# Main Function\n",
    "from torchsummary import summary\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Data Preparation\n",
    "    data_dir = \"output\"\n",
    "    image_dir = os.path.join(data_dir, \"processed_images\")\n",
    "    annotations_file = os.path.join(data_dir, \"ocr/detailed_results.json\")\n",
    "    dataset = HandwritingDataset(image_dir, annotations_file)\n",
    "    print(f\"Total samples: {len(dataset)}\")\n",
    "    \n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Model Initialization\n",
    "    model = HandwritingRecognitionModel(\n",
    "        vocab_size=dataset.vocab_size,\n",
    "        sequence_length=dataset.max_length\n",
    "    ).to(device)\n",
    "    \n",
    "    # Print Model Summary\n",
    "    print(\"\\nModel Summary:\")\n",
    "    summary(model, input_size=(3, 224, 224))\n",
    "    \n",
    "    # Training Setup\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    num_epochs = 15\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'char_to_idx': dataset.char_to_idx,\n",
    "                'idx_to_char': dataset.idx_to_char\n",
    "            }, 'best_model.pth')\n",
    "            print(\"Saved best model\")\n",
    "            \n",
    "            # Sample Predictions\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                batch = next(iter(val_loader))\n",
    "                images = batch['image'].to(device)\n",
    "                logits = model(images)\n",
    "                predictions = decode_prediction(logits, dataset.idx_to_char)\n",
    "                print(\"\\nSample Predictions:\")\n",
    "                for i, pred in enumerate(predictions[:3]):  # Show first 3 predictions\n",
    "                    print(f\"Predicted: {pred}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 112\n",
      "Generating analysis report...\n",
      "Analyzing model performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing performance: 100%|██████████| 27/27 [00:56<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic Statistics:\n",
      "                               Value\n",
      "Total Samples             427.000000\n",
      "Average Edit Distance      91.201405\n",
      "Average Similarity Ratio    0.287489\n",
      "Exact Match Rate            0.000000\n",
      "Character Accuracy          0.287489\n",
      "\n",
      "Worst Examples:\n",
      "                                        predicted_text  \\\n",
      "371  ee                                            ...   \n",
      "383  ee                                            ...   \n",
      "104  ee                                            ...   \n",
      "90   ee                                            ...   \n",
      "228  ee                                            ...   \n",
      "170  ee                                            ...   \n",
      "115  ee                                            ...   \n",
      "187  ee                                            ...   \n",
      "97   ee                                            ...   \n",
      "226  ee                                            ...   \n",
      "\n",
      "                                             true_text  similarity_ratio  \n",
      "371  Fen SOE agian Tae nD meg Gea eA NO eg re HEROE...          0.140625  \n",
      "383  Osa. by che, dae wetts as bela Pech D tect. ce...          0.179688  \n",
      "104  Vika\\nIF\\nVi le tel cre. Chi bag Cetsa <te ¥f ...          0.187500  \n",
      "90   lar) Cescaen. adpetnengy Darnred VE AMMG LEY o...          0.195312  \n",
      "228  LE8\\nJit a ABM Réee Of di Litre? tha Cecefit-o...          0.203125  \n",
      "170  a 29: sarzcaeg SO Ge hen coclh ' . ee fo ee\\n1...          0.203125  \n",
      "115  PS Ht flare alee ore.cte , PAE. eeeliiaiaiass:...          0.210938  \n",
      "187  | ie pes y _ Sete is PE err ee\\nBh Aacate Fite...          0.210938  \n",
      "97   Soha maenwa.t uharar eee ee\\nPperasier Seca Se...          0.210938  \n",
      "226  BAZ\\nLOT\\nPsp <a. oe GE ER: ‘ oe Rae a y “\\nSe...          0.210938  \n",
      "\n",
      "Best Examples:\n",
      "                                        predicted_text  \\\n",
      "238  ee                                            ...   \n",
      "107  ee                                            ...   \n",
      "380  ee                                            ...   \n",
      "262  ee                                            ...   \n",
      "48   ee                                            ...   \n",
      "311  ee                                            ...   \n",
      "292  ee                                            ...   \n",
      "23   ee                                            ...   \n",
      "61   ee                                            ...   \n",
      "319  ee                                            ...   \n",
      "\n",
      "                                             true_text  similarity_ratio  \n",
      "238  Tee a oe 4 :\\neas oS ae ae ee ae ae eee a :\\nS...          0.335938  \n",
      "107  pe) a\\n| ae - 7 :\\n- ce oS ;\\nSe — : ee eG oe ...          0.335938  \n",
      "380  a '\\n. a rf\\na. | :\\n— |\\na | |\\noe a 7, 2\\na ...          0.335938  \n",
      "262  ee oO Se\\noie a os . a\\ni ue A So as\\n—. ce o ...          0.335938  \n",
      "48   Se oe S 8\\noi oS oo oe 2\\n2 7 — oo oo\\n2 a oo\\...          0.335938  \n",
      "311  : — L\\n| L\\ny oo Bs SS ee y a os 5\\n: a i. 3 a...          0.335938  \n",
      "292  ‘ . :\\n— _\\n7 Le a a . :\\ni a a — .\\n— oO oe n...          0.343750  \n",
      "23   . — - L\\ncoe - : |\\n— — a ee A :\\n- - ce :\\noy...          0.343750  \n",
      "61   ee\\n's a\\na oo :\\naS 8 : os a 4\\nJ ee ae a x\\n...          0.351562  \n",
      "319  Gg — | :\\nve a : a :\\na . a a a\\na . J ] , | :...          0.367188  \n",
      "\n",
      "Model Performance Summary:\n",
      "--------------------------------------------------\n",
      "Total Samples: 427.0000\n",
      "Average Edit Distance: 91.2014\n",
      "Average Similarity Ratio: 0.2875\n",
      "Exact Match Rate: 0.0000\n",
      "Character Accuracy: 0.2875\n",
      "\n",
      "Detailed results saved in 'analysis_results' directory\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "class ModelAnalyzer:\n",
    "    def __init__(self, model_path, test_loader, device, idx_to_char):\n",
    "        \"\"\"\n",
    "        Initialize the analyzer with model and data\n",
    "        \"\"\"\n",
    "        self.checkpoint = torch.load(model_path)\n",
    "        self.model = HandwritingRecognitionModel(\n",
    "            vocab_size=len(idx_to_char),\n",
    "            sequence_length=128\n",
    "        ).to(device)\n",
    "        self.model.load_state_dict(self.checkpoint['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        self.idx_to_char = idx_to_char\n",
    "        \n",
    "    def decode_text(self, tensor):\n",
    "        \"\"\"Convert tensor predictions to text\"\"\"\n",
    "        indices = tensor.argmax(dim=-1)\n",
    "        text = ''.join([self.idx_to_char[idx.item()] for idx in indices if idx.item() != 0])\n",
    "        return text\n",
    "        \n",
    "    def calculate_text_metrics(self, pred_text, true_text):\n",
    "        \"\"\"Calculate various text similarity metrics\"\"\"\n",
    "        return {\n",
    "            'edit_distance': edit_distance(pred_text, true_text),\n",
    "            'similarity_ratio': similarity_ratio(pred_text, true_text),\n",
    "            'length_diff': abs(len(pred_text) - len(true_text)),\n",
    "            'exact_match': pred_text == true_text\n",
    "        }\n",
    "        \n",
    "    def analyze_model_performance(self):\n",
    "        \"\"\"Perform comprehensive model analysis\"\"\"\n",
    "        results = []\n",
    "        character_errors = defaultdict(int)\n",
    "        total_chars = 0\n",
    "        correct_chars = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.test_loader, desc=\"Analyzing performance\"):\n",
    "                images = batch['image'].to(self.device)\n",
    "                true_texts = batch['text']\n",
    "                \n",
    "                logits = self.model(images)\n",
    "                \n",
    "                for i in range(len(images)):\n",
    "                    pred_text = self.decode_text(logits[i])\n",
    "                    true_text = ''.join([self.idx_to_char[idx.item()] \n",
    "                                       for idx in true_texts[i] if idx.item() != 0])\n",
    "                    \n",
    "                    metrics = self.calculate_text_metrics(pred_text, true_text)\n",
    "                    \n",
    "                    for pred_char, true_char in zip(pred_text, true_text):\n",
    "                        total_chars += 1\n",
    "                        if pred_char == true_char:\n",
    "                            correct_chars += 1\n",
    "                        else:\n",
    "                            character_errors[f\"{true_char}->{pred_char}\"] += 1\n",
    "                    \n",
    "                    results.append({\n",
    "                        'predicted_text': pred_text,\n",
    "                        'true_text': true_text,\n",
    "                        'text_length': len(true_text),\n",
    "                        **metrics\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(results), character_errors, correct_chars / total_chars\n",
    "\n",
    "    def generate_analysis_report(self, save_dir='analysis_results'):\n",
    "        \"\"\"Generate comprehensive analysis report with visualizations\"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"Analyzing model performance...\")\n",
    "        results_df, char_errors, char_accuracy = self.analyze_model_performance()\n",
    "        \n",
    "        basic_stats = {\n",
    "            'Total Samples': len(results_df),\n",
    "            'Average Edit Distance': results_df['edit_distance'].mean(),\n",
    "            'Average Similarity Ratio': results_df['similarity_ratio'].mean(),\n",
    "            'Exact Match Rate': results_df['exact_match'].mean(),\n",
    "            'Character Accuracy': char_accuracy\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(save_dir, 'basic_stats.json'), 'w') as f:\n",
    "            json.dump(basic_stats, f, indent=4)\n",
    "        \n",
    "        print(\"\\nBasic Statistics:\")\n",
    "        print(pd.DataFrame.from_dict(basic_stats, orient='index', columns=['Value']))\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # Boxplot for Edit Distance\n",
    "        plt.subplot(2, 2, 1)\n",
    "        sns.boxplot(data=results_df, x='edit_distance', color='skyblue')\n",
    "        plt.title('Boxplot of Edit Distances')\n",
    "        plt.xlabel('Edit Distance')\n",
    "        \n",
    "        # Violin plot for Text Length vs Similarity Ratio\n",
    "        plt.subplot(2, 2, 2)\n",
    "        sns.violinplot(data=results_df, x='text_length', y='similarity_ratio', scale='width', palette='muted')\n",
    "        plt.title('Text Length vs Similarity Ratio')\n",
    "        plt.xlabel('Text Length')\n",
    "        plt.ylabel('Similarity Ratio')\n",
    "        \n",
    "        # Heatmap for Character Error Types\n",
    "        plt.subplot(2, 2, 3)\n",
    "        char_errors_df = pd.DataFrame(\n",
    "            list(char_errors.items()),\n",
    "            columns=['Error_Type', 'Count']\n",
    "        ).sort_values('Count', ascending=False).head(10)\n",
    "        sns.heatmap(char_errors_df.pivot_table(index='Error_Type', values='Count', aggfunc='sum'), annot=True, fmt='d', cmap='YlGnBu')\n",
    "        plt.title('Top 10 Character Error Types')\n",
    "        plt.xlabel('Frequency')\n",
    "        \n",
    "        # Boxplot for Length Differences\n",
    "        plt.subplot(2, 2, 4)\n",
    "        sns.boxplot(data=results_df, x='length_diff', color='coral')\n",
    "        plt.title('Boxplot of Length Differences')\n",
    "        plt.xlabel('Length Difference')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, 'analysis_plots.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Save detailed results\n",
    "        results_df.to_csv(os.path.join(save_dir, 'detailed_results.csv'), index=False)\n",
    "        \n",
    "        # Display best and worst examples\n",
    "        examples = results_df.sort_values('similarity_ratio')\n",
    "        worst_examples = examples.head(10)\n",
    "        best_examples = examples.tail(10)\n",
    "        \n",
    "        print(\"\\nWorst Examples:\")\n",
    "        print(worst_examples[['predicted_text', 'true_text', 'similarity_ratio']])\n",
    "        \n",
    "        print(\"\\nBest Examples:\")\n",
    "        print(best_examples[['predicted_text', 'true_text', 'similarity_ratio']])\n",
    "        \n",
    "        examples_report = {\n",
    "            'worst_cases': worst_examples.to_dict('records'),\n",
    "            'best_cases': best_examples.to_dict('records')\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(save_dir, 'example_cases.json'), 'w') as f:\n",
    "            json.dump(examples_report, f, indent=4)\n",
    "        \n",
    "        return basic_stats, results_df\n",
    "    \n",
    "    def categorize_error(self, pred, true):\n",
    "        \"\"\"Categorize the type of error\"\"\"\n",
    "        if len(pred) > len(true):\n",
    "            return 'insertion'\n",
    "        elif len(pred) < len(true):\n",
    "            return 'deletion'\n",
    "        else:\n",
    "            return 'substitution'\n",
    "\n",
    "def analyze_model_results():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model_path = 'best_model.pth'\n",
    "    \n",
    "    test_dataset = HandwritingDataset(\n",
    "        image_dir=\"output/processed_images\",\n",
    "        annotations_file=\"output/ocr/detailed_results.json\"\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    analyzer = ModelAnalyzer(\n",
    "        model_path=model_path,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        idx_to_char=test_dataset.idx_to_char\n",
    "    )\n",
    "    \n",
    "    print(\"Generating analysis report...\")\n",
    "    stats, results = analyzer.generate_analysis_report()\n",
    "    \n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nDetailed results saved in 'analysis_results' directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_model_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
